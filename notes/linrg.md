# Linear Regression

线性回归是最基础的回归模型，用于预测连续目标变量 $y$ 与特征 $X$ 之间的线性关系。它假设特征的贡献可以线性叠加，剩余无法解释的部分由噪声承担。

## 数学模型

核心模型为

$$ y = Xw + b + \epsilon $$

其中 $w$ 是权重向量，$b$ 是截距（bias/intercept），$\epsilon$ 是误差项。

线性回归的拟合目标是最小化平方误差：

$$ \min_{w,b} \; \frac{1}{2n}\|y - Xw - b\|_2^2 $$

平方误差的意义在于对较大偏差给予更强惩罚，同时把问题变成二次凸优化，确保存在全局最优解。该目标也隐含了误差项均值为 0、分布对称的统计假设，从而使模型具有可解释性。

在满足高斯-马尔可夫假设的前提下，OLS 的解 $\hat{w}$ 是 BLUE（最优线性无偏估计），即在所有线性无偏估计中方差最小。

## 模型假设

线性回归依赖以下假设：$y$ 与 $X$ 之间存在线性关系；误差项相互独立、均值为 0、方差恒定（同方差性）；特征之间不存在严格的多重共线性。当假设不满足时，估计量的性质可能变差，正则化可在一定程度上缓解共线性问题。

## 模型特点

线性回归的优点在于可解释性强（权重直接反映特征贡献）、计算高效、有闭式解、易于通过正则化控制复杂度。其局限性是只能拟合线性关系，对非线性模式无能为力，且对异常值较为敏感。适用于特征与目标之间存在近似线性关系、且需要模型可解释性的场景。

## L1 / L2 正则

正则化的目的，是在拟合数据之外约束参数规模，避免模型把噪声当成规律，从而提升稳定性与泛化能力。

L2 正则（Ridge）在目标函数中加入 $\|w\|_2^2$：

$$ \min_{w,b} \; \frac{1}{2n}\|y - Xw - b\|_2^2 + \frac{\lambda}{2}\|w\|_2^2 $$

它会让所有参数整体缩小，但通常不会变成严格的 0。几何上可以理解为偏好更小的解；统计上相当于给参数施加高斯先验；数值上等价于在 $X^T X$ 上加 $\lambda I$，缓解共线性并改善条件数。

L1 正则（Lasso）加入 $\|w\|_1$：

$$ \min_{w,b} \; \frac{1}{2n}\|y - Xw - b\|_2^2 + \lambda \|w\|_1 $$

L1 的惩罚在零点有尖角，会促使部分参数直接变为 0，从而形成稀疏解。稀疏意味着模型自动选择部分特征，其余特征被舍弃，因此 Lasso 常用于特征选择。

## 求解策略

根据正则化类型选择不同的求解策略，关键区别在于目标函数是否光滑。

当没有正则或使用 L2 正则时，目标函数为二次凸函数，可通过正规方程求解。关键公式为

$$ w = (X^T X + \alpha I)^{-1} X^T y $$

其中 $\alpha=0$ 对应 OLS，$\alpha>0$ 对应 Ridge。实现上构造 $X^T X$ 与 $X^T y$，加入正则项后求解线性方程组，避免直接求逆造成的不稳定。

当 $X^T X$ 接近奇异或条件数较大时，可用 SVD 分解提升数值稳定性。设 $X = U\Sigma V^T$，则

$$ w = V \Sigma^+ U^T y $$

其中 $\Sigma^+$ 是对角矩阵的伪逆。SVD 能自动处理奇异情况，是 OLS 的稳健实现方式。

当使用 L1 正则时，目标函数在 0 处不可导，闭式解不可用，因此采用坐标下降。核心更新形式为

$$ w_j \leftarrow \frac{S(\rho_j, \alpha)}{\|x_j\|^2} $$

其中 $\rho_j$ 表示特征与残差的相关性，$S(\cdot)$ 是软阈值算子。相关性不足时权重会被压为 0，相关性足够大时保留并缩小。算法在各维度间循环更新，直到变化量足够小或达到迭代上限。

对于大规模数据，可用梯度下降或其随机变种（SGD）迭代求解：

$$ w \leftarrow w - \eta \cdot \nabla_w L(w) $$

其中 $\eta$ 是学习率。梯度下降无需构造 $X^T X$，内存开销更低，适合样本量或特征维度很大的场景。
