# Naive Bayes

朴素贝叶斯是基于贝叶斯定理和特征条件独立假设的概率分类模型。它通过计算后验概率进行分类，尽管假设强但实践中表现优异，尤其适用于文本分类等高维问题。

## 数学模型

贝叶斯定理给出后验概率：

$$ P(y|x) = \frac{P(y) P(x|y)}{P(x)} $$

其中 $P(y)$ 是先验概率，$P(x|y)$ 是似然，$P(x)$ 是证据。分类决策为

$$ \hat{y} = \arg\max_{y} P(y) \prod_{j=1}^d P(x^{(j)}|y) $$

朴素假设是给定类别下各特征条件独立，因此联合似然可分解为边缘似然的乘积。$P(x)$ 对所有类别相同，可省略。

## 模型变体

不同变体对 $P(x^{(j)}|y)$ 采用不同分布假设。

多项式朴素贝叶斯假设特征服从多项式分布，适用于离散计数数据（如词频）：

$$ P(x^{(j)}|y) = \frac{N_{yj} + \alpha}{N_y + \alpha d} $$

其中 $N_{yj}$ 是类别 $y$ 中特征 $j$ 的总出现次数，$N_y$ 是类别 $y$ 的总词数，$\alpha$ 是拉普拉斯平滑参数。

高斯朴素贝叶斯假设连续特征服从正态分布：

$$ P(x^{(j)}|y) = \frac{1}{\sqrt{2\pi\sigma_{yj}^2}} \exp\left(-\frac{(x^{(j)} - \mu_{yj})^2}{2\sigma_{yj}^2}\right) $$

其中 $\mu_{yj}$ 和 $\sigma_{yj}^2$ 是类别 $y$ 下特征 $j$ 的均值和方差。

伯努利朴素贝叶斯适用于二值特征，建模特征是否出现而非出现次数。

## 模型假设

朴素贝叶斯的核心假设是特征条件独立。这一假设在现实中很少成立，但模型仍常表现良好，原因是分类任务只关心后验概率的相对大小而非绝对值。此外假设特征对类别的影响可加，先验分布均匀或可从数据估计。

## 模型特点

朴素贝叶斯的优点在于计算高效（参数估计只需遍历数据一次）、对小样本表现稳定、对缺失数据不敏感、可解释性强（后验概率有明确意义）、天然支持多分类。其局限性是条件独立假设过强、对特征相关性建模能力弱、高斯假设对非正态数据可能不适用。适用于文本分类、垃圾邮件过滤、情感分析等高维稀疏问题。

## 平滑技术

平滑用于处理训练集中未出现的特征 - 类别组合，避免零概率问题。

拉普拉斯平滑（加一平滑）是最简单方法：

$$ P(x^{(j)}|y) = \frac{\text{count}(x^{(j)}, y) + \alpha}{\text{count}(y) + \alpha d} $$

其中 $\alpha=1$ 是标准拉普拉斯平滑，$\alpha \in (0,1)$ 是利德斯通平滑。较大的 $\alpha$ 使概率分布更均匀，较小的 $\alpha$ 更接近经验频率。

平滑参数 $\alpha$ 可通过交叉验证选择，文本分类中常用 $\alpha=1.0$。

## 求解策略

朴素贝叶斯的参数估计采用极大似然估计（MLE）或极大后验估计（MAP）。

训练阶段统计各类别的先验概率 $P(y)$ 和条件概率 $P(x^{(j)}|y)$。离散特征直接计数，连续特征估计均值和方差。加入平滑项避免零概率。

预测阶段对每个类别计算未归一化的后验概率：

$$ \log P(y|x) \propto \log P(y) + \sum_{j=1}^d \log P(x^{(j)}|y) $$

使用对数求和避免数值下溢。选择后验概率最大的类别作为预测结果。

训练时间复杂度为 $O(nd)$，预测为 $O(cd)$，其中 $c$ 是类别数。
