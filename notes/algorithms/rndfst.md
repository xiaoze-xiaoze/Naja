# Random Forest

随机森林是基于 Bagging 的集成学习模型，通过构建多棵决策树并综合其预测结果来提高泛化性能。它结合了装袋采样和特征随机选择，有效降低了单棵树的方差。

## 数学模型

随机森林由 $B$ 棵决策树 $\{T_1, T_2, \ldots, T_B\}$ 组成，每棵树在训练时从原始数据集中有放回抽取 $n$ 个样本（Bootstrap 采样）。预测时：

分类任务采用多数投票：

$$ \hat{y} = \arg\max_{c} \sum_{b=1}^B \mathbb{I}(T_b(x) = c) $$

回归任务采用平均：

$$ \hat{y} = \frac{1}{B} \sum_{b=1}^B T_b(x) $$

关键创新是在每次分裂时，从 $d$ 个特征中随机选择 $m$ 个候选特征（$m < d$），而非考察全部特征。这一机制增加了树之间的多样性，降低了集成的相关性。

## 模型假设

随机森林假设多棵弱学习器的集成能够降低方差；树之间的多样性有助于提升泛化能力；Bootstrap 采样能够近似原始数据分布。对特征之间的交互关系有一定建模能力，但对强线性关系拟合能力有限。

## 模型特点

随机森林的优点在于泛化性能优异（通常优于单棵树）、对过拟合有较强抵抗力、可处理高维数据、对缺失值和异常值鲁棒、可评估特征重要性、对特征缩放不敏感、可并行训练。其局限性是可解释性较差（相比单棵树）、模型体积大（需存储多棵树）、预测速度较慢、对某些噪声较多的数据集可能表现不佳。

## 特征重要性

随机森林提供两种特征重要性评估方法。

基于不纯度的重要性（Mean Decrease Impurity, MDI）计算特征在所有树的所有节点中带来的不纯度减少总量的平均值：

$$ I_j = \frac{1}{B} \sum_{b=1}^B \sum_{t \in T_b} \mathbb{I}(j_t = j) \cdot \Delta i(t) $$

其中 $j_t$ 是节点 $t$ 的分裂特征，$\Delta i(t)$ 是该分裂的不纯度减少量。MDI 计算高效但有偏（偏向高基数特征）。

基于排列的重要性（Mean Decrease Accuracy, MDA）通过随机打乱某特征的值，观察模型性能下降程度：

$$ I_j = \frac{1}{B} \sum_{b=1}^B \left[ Acc_b - Acc_b^{\text{perm}(j)} \right] $$

MDA 无偏但计算开销大（需重复预测）。

## 超参数

随机森林的关键超参数包括：

树的数量 `n_estimators`：较大的值提升性能但增加计算开销，通常 100-500 之间足够。

树的最大深度 `max_depth`：限制单棵树的复杂度，较深的树方差大但偏差小，随机森林中通常允许树充分生长。

每次分裂的候选特征数 `max_features`：分类问题常用 $\sqrt{d}$ 或 $\log_2 d$，回归问题常用 $d/3$ 或 $d/2$。较小的值增加多样性但可能增加偏差。

叶节点最小样本数 `min_samples_leaf`：控制平滑程度，较大的值使模型更保守。

Bootstrap 采样比例 `max_samples`：可设置为小于 1 的值引入更多随机性。

## 求解策略

随机森林的训练过程如下：

对每棵树 $b=1,\ldots,B$：

1. 从训练集中有放回抽取 $n$ 个样本形成 Bootstrap 数据集 $D_b$
2. 用 $D_b$ 训练决策树 $T_b$，每次分裂时从 $m$ 个随机选择的特征中选择最优分裂点
3. 树生长到最大深度或满足其他停止条件

预测时对分类任务汇总各树投票，对回归任务平均各树预测。

袋外误差（Out-of-Bag Error）是利用未参与某棵树训练的样本（约占 36.8%）评估该树的泛化误差，可作为交叉验证的无偏估计，无需单独划分验证集。

训练可完全并行化，时间复杂度为 $O(B \cdot nd \log n)$。
