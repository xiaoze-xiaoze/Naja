# Gradient Boosting Machine

GBM（Gradient Boosting Machine）是基于 Gradient Boosting 的朴素实现，通过加法模型和前向分步算法构建多棵回归树。每棵树拟合当前模型的负梯度（伪残差）。

## 数学模型

GBM 的预测函数是 $M$ 棵树的加权和：

$$ \hat{y}_i = \sum_{m=1}^M \rho_m h_m(x_i) $$

其中 $h_m$ 是第 $m$ 棵回归树，$\rho_m$ 是学习率（shrinkage）。

优化目标为：

$$ \min \sum_{i=1}^n L(y_i, \hat{y}_i) $$

采用前向分步算法，第 $m$ 步的预测为：

$$ \hat{y}_i^{(m)} = \hat{y}_i^{(m-1)} + \rho_m h_m(x_i) $$

每一步计算负梯度（伪残差）：

$$ r_{im} = -\left[ \frac{\partial L(y_i, \hat{y}_i)}{\partial \hat{y}_i} \right]_{\hat{y}=\hat{y}^{(m-1)}} $$

然后拟合回归树 $h_m$ 到伪残差 $\{r_{im}\}$，得到叶节点区域 $R_{jm}$，并计算最优权重：

$$ \rho_{jm} = \arg\min_{\rho} \sum_{x_i \in R_{jm}} L(y_i, \hat{y}_i^{(m-1)} + \rho) $$

对于平方损失，$\rho_{jm}$ 就是叶节点内残差的均值。

## 模型假设

GBM 假设数据可被多棵树的加法模型有效拟合；负梯度能够指导模型改进方向；单步优化能够累积逼近最优解。对特征缩放不敏感，但对异常值和标签噪声较敏感。

## 模型特点

GBM 的优点在于预测精度高、支持多种损失函数、实现简单直观、可处理非线性关系。其局限性是训练时间较长（串行依赖）、超参数需调优、对异常值敏感、可解释性较差。

## 学习率

学习率 $\rho$（又称 shrinkage）控制每棵树的贡献：

$$ \hat{y}_i^{(m)} = \hat{y}_i^{(m-1)} + \rho \cdot h_m(x_i) $$

较小的学习率（如 0.01-0.1）需要更多的树，但通常能获得更好的泛化性能。学习率与树的数量之间存在权衡：较小的学习率需要更多的迭代次数，但过小的学习率可能导致欠拟合。

## 求解策略

GBM 采用贪心算法构建决策树：

1. 计算当前模型对所有样本的负梯度（伪残差）
2. 用回归树拟合伪残差
3. 对每个叶节点，求解使该节点损失最小的权重
4. 更新模型：$F_m = F_{m-1} + \rho \cdot h_m$

树构建时，选择使残差平方和下降最大的分裂点。对于回归任务（平方损失），伪残差就是 $y_i - \hat{y}_i^{(m-1)}$。

Early stopping 可用于防止过拟合：当验证集损失连续多轮不下降时停止训练。
