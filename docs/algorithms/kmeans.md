# K-Means

K-Means 是最经典的聚类算法，通过迭代优化将数据划分为 $K$ 个簇，使簇内样本到簇中心的距离平方和最小。它简单高效，广泛应用于数据探索和预处理。

## 数学模型

K-Means 的优化目标是最小化簇内平方和（Within-Cluster Sum of Squares, WCSS）：

$$ \min_{C_1,\ldots,C_K} \sum_{k=1}^K \sum_{x_i \in C_k} \|x_i - \mu_k\|^2 $$

其中 $C_k$ 是第 $k$ 个簇的样本集合，$\mu_k = \frac{1}{|C_k|}\sum_{x_i \in C_k} x_i$ 是簇中心。

采用 Lloyd 算法迭代求解：

1. 初始化 $K$ 个簇中心 $\mu_1, \ldots, \mu_K$
2. 分配阶段：将每个样本分配到最近的簇中心
   $$ c_i = \arg\min_k \|x_i - \mu_k\|^2 $$
3. 更新阶段：重新计算每个簇的中心
   $$ \mu_k = \frac{1}{|C_k|} \sum_{i: c_i=k} x_i $$
4. 重复步骤 2-3 直到收敛（分配不再变化或目标函数变化小于阈值）

算法保证目标函数单调下降，但可能收敛到局部最优。

## 模型假设

K-Means 假设簇是凸的且各向同性（球形）；簇的大小相近；簇密度相近；$K$ 值已知或可估计。对非凸形状、大小差异大、密度差异大的簇效果不佳。

## 模型特点

K-Means 的优点在于实现简单、计算高效（时间复杂度 $O(nKd \cdot \text{iter})$）、内存开销小、可解释性强（簇中心代表簇的典型样本）、易于并行。其局限性是需预先指定 $K$、对初始值敏感、只能发现凸形簇、对异常值敏感（均值易受极端值影响）、对特征缩放敏感。

## K 值选择

选择合适的 $K$ 是 K-Means 的关键挑战。

肘部法则（Elbow Method）绘制 WCSS 随 $K$ 变化的曲线，选择 WCSS 下降速率显著减缓的"肘部"点。该点之后增加 $K$ 带来的改善有限。

轮廓系数（Silhouette Score）衡量簇的紧密度和分离度：

$$ s(i) = \frac{b(i) - a(i)}{\max\{a(i), b(i)\}} $$

其中 $a(i)$ 是样本 $i$ 到同簇其他样本的平均距离，$b(i)$ 是样本 $i$ 到最近其他簇的平均距离。轮廓系数越接近 1 越好，选择使平均轮廓系数最大的 $K$。

Gap 统计量比较实际 WCSS 与参考分布（均匀分布）的 WCSS 的差距，选择使 Gap 最大的 $K$。

## 初始化策略

初始中心的选择影响最终结果和收敛速度。

随机初始化随机选择 $K$ 个样本作为初始中心，简单但结果不稳定，需多次运行取最优。

K-Means++ 改进初始化：第一个中心随机选择；后续每个中心以概率 $D(x)^2$ 选择，其中 $D(x)$ 是样本 $x$ 到已选中心的最近距离。K-Means++ 使初始中心分散，理论保证期望解与最优解的差距为 $O(\log K)$。

层次聚类初始化先用层次聚类得到 $K$ 个簇，用其中心作为 K-Means 初始值，稳定性好但计算开销大。

## 加速策略

大规模数据需采用加速方法。

Mini-Batch K-Means 每次迭代用一小批样本更新簇中心，大幅降低计算成本，适合在线学习和超大规模数据。

KD 树或球树加速最近邻搜索，避免计算所有样本到所有中心的距离。

三角不等式利用距离的三角不等式性质，跳过不必要的距离计算。

## 变体

K-Medians 用中位数而非均值作为簇中心，对异常值更鲁棒。

K-Medoids 用实际样本（中心点）代表簇，可处理非欧几里得距离。

Fuzzy C-Means 允许样本以不同隶属度属于多个簇，是软聚类方法。

Spherical K-Means 对归一化数据用余弦相似度替代欧氏距离，适合文本聚类。
