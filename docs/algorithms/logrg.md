# Logistic Regression

逻辑回归是最基础的分类模型，用于预测离散目标变量 $y$ 与特征 $X$ 之间的关系。它在线性模型的基础上通过 Sigmoid 函数将输出映射到 (0,1) 区间，解释为样本属于正类的概率。

## 数学模型

核心模型为

$$ P(y=1|X) = \sigma(Xw + b) = \frac{1}{1 + e^{-(Xw + b)}} $$

其中 $w$ 是权重向量，$b$ 是截距，$\sigma(\cdot)$ 是 Sigmoid 函数。Sigmoid 函数将任意实数压缩到 (0,1) 之间，形成平滑的概率输出。

逻辑回归的拟合目标是最大化对数似然，等价于最小化交叉熵损失：

$$ \min_{w,b} \; -\frac{1}{n}\sum_{i=1}^n \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right] $$

其中 $p_i = \sigma(X_i w + b)$。交叉熵损失对错误预测给予更强惩罚，且是凸函数，确保存在全局最优解。

## 模型假设

逻辑回归依赖以下假设：对数几率 $\log\frac{P(y=1)}{P(y=0)}$ 与特征之间存在线性关系；样本之间相互独立；特征之间不存在严重的多重共线性。当假设不满足时，模型的校准性可能变差，正则化可在一定程度上缓解共线性问题。

## 模型特点

逻辑回归的优点在于输出具有概率意义、可解释性强（权重符号和大小反映特征对正类的贡献）、计算高效、易于通过正则化控制复杂度。其局限性是只能拟合线性决策边界，对非线性模式无能为力。适用于特征与对数几率之间存在近似线性关系、且需要概率输出的场景。

## L1 / L2 正则

正则化的目的，是在拟合数据之外约束参数规模，避免模型把噪声当成规律，从而提升稳定性与泛化能力。

L2 正则（Ridge）在目标函数中加入 $\|w\|_2^2$：

$$ \min_{w,b} \; -\frac{1}{n}\sum_{i=1}^n \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right] + \frac{\lambda}{2}\|w\|_2^2 $$

它会让所有参数整体缩小，但通常不会变成严格的 0。几何上可以理解为偏好更小的解；统计上相当于给参数施加高斯先验；数值上等价于在海森矩阵上加 $\lambda I$，缓解共线性并改善条件数。

L1 正则（Lasso）加入 $\|w\|_1$：

$$ \min_{w,b} \; -\frac{1}{n}\sum_{i=1}^n \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right] + \lambda \|w\|_1 $$

L1 的惩罚在零点有尖角，会促使部分参数直接变为 0，从而形成稀疏解。稀疏意味着模型自动选择部分特征，其余特征被舍弃，因此 Lasso 常用于特征选择。

## 求解策略

逻辑回归没有闭式解，必须采用迭代优化算法。关键区别在于目标函数是否光滑以及是否使用二阶信息。

当没有正则或使用 L2 正则时，目标函数是光滑凸函数，可通过牛顿法或拟牛顿法求解。牛顿法利用海森矩阵进行二阶更新：

$$ w \leftarrow w - H^{-1} \nabla_w L(w) $$

其中 $H$ 是海森矩阵。拟牛顿法（如 L-BFGS）用低秩近似替代海森矩阵，在保持快速收敛的同时降低内存开销，是中小规模问题的首选。

当使用 L1 正则时，目标函数在 0 处不可导，因此采用坐标下降或近端梯度方法。坐标下降的核心更新形式为

$$ w_j \leftarrow \text{Prox}_{\alpha\lambda}\left( w_j - \alpha \frac{\partial L}{\partial w_j} \right) $$

其中 $\text{Prox}$ 是软阈值算子。相关性不足时权重会被压为 0，相关性足够大时保留并缩小。算法在各维度间循环更新，直到变化量足够小或达到迭代上限。

对于大规模数据，可用梯度下降或其随机变种（SGD）迭代求解：

$$ w \leftarrow w - \eta \cdot \nabla_w L(w) $$

其中 $\eta$ 是学习率。梯度下降无需构造海森矩阵，内存开销更低，适合样本量或特征维度很大的场景。
