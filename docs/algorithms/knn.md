# K-Nearest Neighbors

K 近邻（KNN）是最简单的惰性学习算法，用于分类和回归任务。它基于"物以类聚"的直觉，用待预测样本的 K 个最近邻的标签进行投票或平均来做出预测。

## 数学模型

KNN 没有显式的训练阶段，预测时直接计算待测样本 $x$ 与所有训练样本 $x_i$ 的距离：

$$ d(x, x_i) = \|x - x_i\|_p = \left( \sum_{j=1}^d |x^{(j)} - x_i^{(j)}|^p \right)^{1/p} $$

其中 $p$ 控制距离类型：$p=2$ 是欧氏距离，$p=1$ 是曼哈顿距离，$p \to \infty$ 是切比雪夫距离。

分类任务采用多数投票：

$$ \hat{y} = \arg\max_{c} \sum_{i \in N_K(x)} \mathbb{I}(y_i = c) $$

其中 $N_K(x)$ 是 $x$ 的 K 个最近邻集合。回归任务采用加权平均：

$$ \hat{y} = \frac{\sum_{i \in N_K(x)} w_i y_i}{\sum_{i \in N_K(x)} w_i} $$

其中权重 $w_i$ 通常取距离的倒数 $w_i = 1/d(x,x_i)$。

## 模型假设

KNN 假设相似输入具有相似输出；特征空间中的距离能够反映样本间的语义相似性；训练数据具有代表性且分布均匀。当数据分布不均匀或存在噪声时，性能可能下降。

## 模型特点

KNN 的优点在于实现简单、无需训练、天然支持多分类、对非线性边界适应性好、可在线学习（直接添加新样本）。其局限性是预测开销大（需计算与所有训练样本的距离）、对高维数据效果差（维数灾难）、对噪声和异常值敏感、需要存储全部训练数据。适用于中小规模、低维、局部结构明显的任务。

## 距离度量

距离度量是 KNN 的核心，选择取决于数据类型和问题特性。

欧氏距离 $p=2$ 是最常用选择，适合连续特征且各维度尺度相近的场景。曼哈顿距离 $p=1$ 对异常值更鲁棒，适合高维稀疏数据。闵可夫斯基距离是通用形式，通过调整 $p$ 适应不同场景。余弦相似度 $\cos\theta = \frac{x \cdot x_i}{\|x\|\|x_i\|}$ 关注方向而非大小，适合文本等高维稀疏数据。马氏距离考虑特征协方差，可处理相关特征，但计算成本较高。

## K 值选择

K 是 KNN 的关键超参数。较小的 K 使决策边界更复杂，模型方差大、偏差小，容易过拟合；较大的 K 使边界更平滑，模型偏差大、方差小，可能欠拟合。极端情况 $K=1$ 完全记忆训练数据，$K=n$ 则总是预测多数类。

K 的选择通常通过交叉验证确定，经验法则是取样本量的平方根附近且为奇数（避免平票）。

## 加速策略

暴力搜索的时间复杂度为 $O(n)$，大规模数据需采用加速方法。

KD 树是二叉树结构，递归地用垂直于坐标轴的超平面分割空间。搜索时从根节点 descent 到叶节点，回溯时检查另一子树是否可能有更近邻。适合低维数据（$d<20$），高维时退化。

球树用超球体而非超矩形划分空间，每个节点存储球心和半径。利用三角不等式剪枝，对某些流形结构数据表现更好。

局部敏感哈希（LSH）将相似样本哈希到同一桶的概率更高，实现近似最近邻搜索，适合超高维数据。
