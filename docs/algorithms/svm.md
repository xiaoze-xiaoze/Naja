# Support Vector Machine

支持向量机（SVM）是强大的监督学习模型，用于分类和回归任务。它通过寻找最大间隔超平面来分离不同类别的样本，核心思想是结构风险最小化。

## 数学模型

原始优化问题为

$$ \min_{w,b} \; \frac{1}{2}\|w\|_2^2 \quad \text{s.t.} \quad y_i(w^T x_i + b) \geq 1, \forall i $$

其中 $w$ 是法向量，$b$ 是偏置。约束条件保证所有样本被正确分类且与超平面的函数间隔至少为 1。

引入松弛变量 $\xi_i \geq 0$ 处理线性不可分情况：

$$ \min_{w,b,\xi} \; \frac{1}{2}\|w\|_2^2 + C\sum_{i=1}^n \xi_i \quad \text{s.t.} \quad y_i(w^T x_i + b) \geq 1-\xi_i $$

其中 $C>0$ 是惩罚参数，控制间隔宽度与误分类之间的权衡。

对偶问题通过拉格朗日乘子法导出：

$$ \max_{\alpha} \; \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j) $$

$$ \text{s.t.} \quad 0 \leq \alpha_i \leq C, \quad \sum_{i=1}^n \alpha_i y_i = 0 $$

其中 $K(\cdot,\cdot)$ 是核函数。只有 $\alpha_i > 0$ 的样本是支持向量，参与最终决策。

## 核函数

核函数将低维非线性问题映射到高维空间，使其线性可分，无需显式计算高维特征。

常用核函数包括：线性核 $K(x_i,x_j)=x_i^T x_j$，适用于高维稀疏数据；多项式核 $K(x_i,x_j)=(\gamma x_i^T x_j + r)^d$，可拟合特征交互；RBF 核 $K(x_i,x_j)=\exp(-\gamma\|x_i-x_j\|^2)$，最通用的非线性核；Sigmoid 核 $K(x_i,x_j)=\tanh(\gamma x_i^T x_j + r)$，形式类似神经网络激活函数。

## 模型假设

SVM 假设存在一个超平面能够有效分离不同类别；支持向量包含了足够的分类信息；核函数能够捕捉数据的内在结构。当类别重叠严重或样本量极大时，性能可能下降。

## 模型特点

SVM 的优点在于泛化能力强（基于结构风险最小化）、对高维数据表现优异、核技巧可处理非线性问题、解具有稀疏性（仅依赖支持向量）。其局限性是对大规模数据训练较慢、对核函数和超参数敏感、多分类需特殊处理。适用于中小规模、高维、非线性可分的分类任务。

## 正则化

惩罚参数 $C$ 是 SVM 的正则化控制项。较大的 $C$ 强调正确分类训练样本，间隔较窄，可能过拟合；较小的 $C$ 允许更多误分类，间隔较宽，可能欠拟合。$C$ 的选择需要在偏差与方差之间取得平衡。

## 求解策略

SVM 的对偶问题是带约束的二次规划问题，常用求解方法包括：

SMO（序列最小优化）是经典算法，每次迭代选择两个拉格朗日乘子 $\alpha_i, \alpha_j$ 固定其他变量，解析求解子问题。更新规则为

$$ \alpha_j^{\text{new}} = \alpha_j^{\text{old}} + \frac{y_j(E_i - E_j)}{\eta} $$

其中 $E_i$ 是预测误差，$\eta$ 是学习步长。算法循环选择违反 KKT 条件最严重的乘子对进行更新，直到收敛。

对于大规模问题，可用坐标下降或随机梯度下降等一阶方法，每次更新单个或少量变量，降低内存开销。
