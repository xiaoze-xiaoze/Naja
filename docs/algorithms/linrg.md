# Linear Regression

线性回归是最基础的回归模型，用于预测连续目标变量 $y$ 与特征 $X$ 之间的线性关系。它假设特征的贡献可以线性叠加，剩余无法解释的部分由噪声承担。

## 数学模型

核心模型为

$$ y = Xw + b + \epsilon $$

其中 $w$ 是权重向量，$b$ 是截距（bias/intercept），$\epsilon$ 是误差项。矩阵写法

$$ y = Xw + b + \epsilon $$

强调了两点：所有样本共享同一组参数；截距相当于一列常数项，表示整体的基线偏移。

线性回归的拟合目标是最小化平方误差：

$$ \min_{w,b} \; \frac{1}{2n}\|y - Xw - b\|_2^2 $$

平方误差的意义在于对较大偏差给予更强惩罚，同时把问题变成二次凸优化，确保存在全局最优解。该目标也隐含了误差项均值为 0、分布对称的统计假设，从而使模型具有可解释性。

## L1 / L2 正则

正则化的目的，是在拟合数据之外约束参数规模，避免模型把噪声当成规律，从而提升稳定性与泛化能力。

L2 正则（Ridge）在目标函数中加入 $\|w\|_2^2$：

$$ \min_{w,b} \; \frac{1}{2n}\|y - Xw - b\|_2^2 + \frac{\lambda}{2}\|w\|_2^2 $$

它会让所有参数整体缩小，但通常不会变成严格的 0。几何上可以理解为偏好更小的解；统计上相当于给参数施加高斯先验；数值上等价于在 $X^T X$ 上加 $\lambda I$，缓解共线性并改善条件数。

L1 正则（Lasso）加入 $\|w\|_1$：

$$ \min_{w,b} \; \frac{1}{2n}\|y - Xw - b\|_2^2 + \lambda \|w\|_1 $$

L1 的惩罚在零点有尖角，会促使部分参数直接变为 0，从而形成稀疏解。稀疏意味着模型自动选择部分特征，其余特征被舍弃，因此 Lasso 常用于特征选择。

## 求解策略

根据正则化类型选择不同的求解策略，关键区别在于目标函数是否光滑。

当没有正则或使用 L2 正则时，目标函数为二次凸函数，可通过正规方程求解。关键公式为

$$ w = (X^T X + \alpha I)^{-1} X^T y $$

其中 $\alpha=0$ 对应 OLS，$\alpha>0$ 对应 Ridge。实现上构造 $X^T X$ 与 $X^T y$，加入正则项后求解线性方程组，避免直接求逆造成的不稳定。

当使用 L1 正则时，目标函数在 0 处不可导，闭式解不可用，因此采用坐标下降。核心更新形式为

$$ w_j \leftarrow \frac{S(\rho_j, \alpha)}{\|x_j\|^2} $$

其中 $\rho_j$ 表示特征与残差的相关性，$S(\cdot)$ 是软阈值算子。相关性不足时权重会被压为 0，相关性足够大时保留并缩小。算法在各维度间循环更新，直到变化量足够小或达到迭代上限。
