# Decision Tree

决策树是直观的树形分类和回归模型。它通过递归地选择最优特征进行分割，将特征空间划分为若干子区域，每个叶节点对应一个预测值。

## 数学模型

决策树将输入空间划分为 $M$ 个互不相交的区域 $R_1, R_2, \ldots, R_M$，预测函数为

$$ f(x) = \sum_{m=1}^M c_m \mathbb{I}(x \in R_m) $$

其中 $c_m$ 是区域 $R_m$ 的预测值。分类树中 $c_m$ 是该区域多数类，回归树中 $c_m$ 是均值。

树的构建采用贪心策略，每次选择最优分割点 $(j,s)$：

$$ \min_{j,s} \left[ \min_{c_1} \sum_{x_i \in R_1(j,s)} L(y_i, c_1) + \min_{c_2} \sum_{x_i \in R_2(j,s)} L(y_i, c_2) \right] $$

其中 $R_1(j,s)=\{x|x^{(j)}\leq s\}$，$R_2(j,s)=\{x|x^{(j)}> s\}$，$L$ 是损失函数。

## 分裂准则

分裂准则衡量分割后子节点的纯度提升程度。

分类树常用基尼不纯度：

$$ Gini(D) = 1 - \sum_{k=1}^K p_k^2 $$

其中 $p_k$ 是类别 $k$ 在节点中的比例。分裂的信息增益为

$$ \Delta Gini = Gini(D) - \sum_{v} \frac{|D_v|}{|D|} Gini(D_v) $$

信息熵是另一选择：

$$ H(D) = -\sum_{k=1}^K p_k \log p_k $$

$$ Gain(D, A) = H(D) - \sum_{v} \frac{|D_v|}{|D|} H(D_v) $$

C4.5 使用信息增益率 $GainRatio = Gain / H_A(D)$ 校正多值特征偏好。

回归树使用方差减少：

$$ Var(D) = \frac{1}{|D|}\sum_{i \in D} (y_i - \bar{y})^2 $$

$$ \Delta Var = Var(D) - \sum_{v} \frac{|D_v|}{|D|} Var(D_v) $$

## 模型假设

决策树假设特征空间可被轴平行超平面有效划分；局部区域内样本具有相似的输出；树的结构能够捕捉特征与目标之间的交互关系。对噪声数据和异常值敏感，容易过拟合。

## 模型特点

决策树的优点在于可解释性强（规则可视化为树形结构）、可处理数值和类别特征、对特征缩放不敏感、可捕捉非线性关系和特征交互、易于处理多输出问题。其局限性是容易过拟合（尤其深树）、对数据扰动敏感（小幅变化可能导致树结构剧变）、难以拟合线性关系、偏向取值较多的特征。

## 剪枝策略

剪枝是控制复杂度、防止过拟合的关键手段。

预剪枝在建树过程中提前停止，策略包括：限制最大深度 `max_depth`；限制叶节点最小样本数 `min_samples_leaf`；限制内部节点再划分所需最小样本数 `min_samples_split`；限制最大叶节点数 `max_leaf_nodes`。预剪枝计算高效但可能欠拟合。

后剪枝先构建完整树，再自底向上合并子树。代价复杂度剪枝（CCP）定义子树 $T$ 的损失为

$$ R_\alpha(T) = R(T) + \alpha |T| $$

其中 $R(T)$ 是训练误差，$|T|$ 是叶节点数，$\alpha$ 是复杂度参数。通过调整 $\alpha$ 生成子树序列，用验证集选择最优子树。后剪枝泛化性能通常更好但计算开销大。

## 求解策略

决策树采用递归分割算法（如 CART、ID3、C4.5）。

CART（分类与回归树）是常用算法，对分类树使用基尼不纯度，对回归树使用方差减少。对每个特征遍历所有可能分割点，选择使不纯度下降最大的分割。递归应用于子节点，直到满足停止条件。

处理类别特征时，对有序类别可直接排序分割，对无序类别需考虑所有二分划分（$2^{k-1}-1$ 种）。处理缺失值时，可将缺失样本分配到所有子节点并按比例加权，或用代理分裂（surrogate split）。

训练时间复杂度约为 $O(nd \log n)$，预测为 $O(\text{depth})$。
