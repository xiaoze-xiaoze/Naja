# Principal Component Analysis

主成分分析（PCA）是最经典的无监督线性降维方法。它通过正交变换将数据投影到方差最大的方向，在保留主要信息的同时降低维度。

## 数学模型

PCA 寻找投影方向 $w$，使投影后方差最大化：

$$ \max_w \text{Var}(Xw) = \max_w w^T \Sigma w \quad \text{s.t.} \quad \|w\|_2^2 = 1 $$

其中 $\Sigma = \frac{1}{n} X^T X$ 是协方差矩阵（假设数据已中心化）。

引入拉格朗日乘子：

$$ \mathcal{L}(w, \lambda) = w^T \Sigma w - \lambda (w^T w - 1) $$

求导得

$$ \frac{\partial \mathcal{L}}{\partial w} = 2\Sigma w - 2\lambda w = 0 $$

$$ \Sigma w = \lambda w $$

最优方向是协方差矩阵的特征向量，对应的特征值 $\lambda$ 是投影后的方差。

第一主成分是最大特征值对应的特征向量 $w_1$，第二主成分是与 $w_1$ 正交且方差次大的方向，依此类推。前 $k$ 个主成分张成最优 $k$ 维子空间。

## 求解方法

PCA 有两种等价的求解方式。

**特征值分解**（Eigendecomposition）

1. 数据中心化：$X \leftarrow X - \bar{X}$
2. 计算协方差矩阵：$\Sigma = \frac{1}{n} X^T X$
3. 对 $\Sigma$ 做特征分解：$\Sigma = W \Lambda W^T$
4. 取前 $k$ 个最大特征值对应的特征向量构成投影矩阵 $W_k$
5. 降维结果：$X_{\text{reduced}} = X W_k$

**奇异值分解**（SVD）

对中心化后的 $X$ 直接做 SVD：

$$ X = U \Sigma V^T $$

其中 $V$ 的列向量是主成分方向，$\Sigma$ 的对角元素是奇异值，与特征值的关系为 $\lambda_i = \sigma_i^2 / n$。

SVD 方法无需显式构造协方差矩阵，数值稳定性更好，是实际实现的首选。

## 模型假设

PCA 假设数据的主要信息集中在高方差方向；线性结构能够有效捕捉数据的内在流形；高斯噪声或各向同性噪声可被低方差方向吸收。对非线性流形结构、离散数据或存在异常值的情况效果有限。

## 模型特点

PCA 的优点在于无监督（无需标签）、计算高效（闭式解）、可去除特征相关性、可压缩数据并降噪、结果可解释（主成分反映数据主要变化模式）、正交变换保持欧氏距离。其局限性是只能捕捉线性关系、对异常值敏感（方差易受极端值影响）、降维后特征失去原始物理意义、需预先确定主成分数量。

## 主成分数量选择

选择主成分数量 $k$ 的常用方法包括：

**累计方差贡献率**

第 $i$ 个主成分的方差贡献率为 $\lambda_i / \sum_j \lambda_j$，累计贡献率为

$$ \text{Explained Variance Ratio} = \frac{\sum_{i=1}^k \lambda_i}{\sum_{j=1}^d \lambda_j} $$

通常选择使累计贡献率达到阈值（如 95% 或 99%）的最小 $k$。

**碎石图**（Scree Plot）

绘制特征值随主成分序号的变化曲线，选择曲线"肘部"对应的 $k$。该点之后特征值下降平缓，新增主成分信息量有限。

**凯泽准则**

保留特征值大于 1（或大于平均特征值）的主成分。该准则适用于标准化后的数据。

**交叉验证**

将数据分为训练集和验证集，在训练集上拟合 PCA，在验证集上评估重构误差，选择使重构误差最小的 $k$。

## 数据预处理

PCA 对特征尺度敏感，预处理至关重要。

**中心化**

PCA 要求数据均值为零，否则第一主成分可能指向均值方向而非最大方差方向。

$$ X \leftarrow X - \bar{X} $$

**标准化**

当特征量纲差异较大时，需标准化使各特征方差为 1：

$$ X^{(j)} \leftarrow \frac{X^{(j)} - \bar{X}^{(j)}}{\sigma_j} $$

标准化后 PCA 等价于对相关性矩阵而非协方差矩阵做特征分解。若特征量纲一致且方差差异有实际意义，可只中心化不标准化。

**异常值处理**

异常值对方差影响极大，应在 PCA 前检测并处理异常值，或使用鲁棒 PCA 变体。

## 变体

**核 PCA**（Kernel PCA）

通过核函数将数据映射到高维空间，在高维空间执行 PCA，可捕捉非线性结构。常用核函数包括 RBF、多项式等。

**稀疏 PCA**（Sparse PCA）

在主成分中加入 L1 正则，使载荷向量稀疏，提高可解释性。

$$ \max_w w^T \Sigma w - \alpha \|w\|_1 \quad \text{s.t.} \quad \|w\|_2^2 = 1 $$

**增量 PCA**（Incremental PCA）

支持流式处理和内存受限场景，逐批更新主成分，无需一次性加载全部数据。

**鲁棒 PCA**（Robust PCA）

将数据分解为低秩部分和稀疏异常部分，对异常值不敏感：

$$ \min_{L,S} \|L\|_* + \lambda \|S\|_1 \quad \text{s.t.} \quad X = L + S $$

其中 $\|\cdot\|_*$ 是核范数（奇异值之和）。
