# Linear Discriminant Analysis

线性判别分析（LDA）是经典的有监督降维和分类方法。它寻找最优投影方向，使类间散度最大化同时类内散度最小化，从而实现类别的最佳分离。

## 数学模型

LDA 定义三个散度矩阵：

类内散度矩阵（Within-class scatter）

$$ S_W = \sum_{k=1}^K \sum_{i \in C_k} (x_i - \mu_k)(x_i - \mu_k)^T $$

其中 $\mu_k$ 是第 $k$ 类的均值向量。

类间散度矩阵（Between-class scatter）

$$ S_B = \sum_{k=1}^K n_k (\mu_k - \mu)(\mu_k - \mu)^T $$

其中 $\mu$ 是全局均值，$n_k$ 是第 $k$ 类的样本数。

总散度矩阵（Total scatter）

$$ S_T = S_W + S_B = \sum_{i=1}^n (x_i - \mu)(x_i - \mu)^T $$

LDA 的优化目标是找到投影矩阵 $W$ 最大化 Rayleigh 商：

$$ \max_W J(W) = \frac{|W^T S_B W|}{|W^T S_W W|} $$

该目标使投影后类间方差与类内方差的比值最大。

## 求解方法

最优投影方向是广义特征值问题的解：

$$ S_B w = \lambda S_W w $$

若 $S_W$ 可逆，等价于

$$ S_W^{-1} S_B w = \lambda w $$

最优 $W$ 由 $S_W^{-1} S_B$ 的前 $d'$ 个最大特征值对应的特征向量组成，其中 $d' \leq \min(K-1, d)$。

求解步骤：
1. 计算各类均值 $\mu_k$ 和全局均值 $\mu$
2. 计算 $S_W$ 和 $S_B$
3. 对 $S_W^{-1} S_B$ 做特征分解
4. 取前 $d'$ 个特征向量构成投影矩阵 $W$
5. 降维结果为 $X_{\text{reduced}} = XW$

## 模型假设

LDA 假设各类数据服从高斯分布；各类具有相同的协方差矩阵（同方差性）；类别可分性主要由均值差异决定。当协方差差异较大或分布非高斯时，性能可能下降。

## 模型特点

LDA 的优点在于有监督（利用类别信息）、可解释性强（投影方向反映类别区分特征）、计算高效（闭式解）、降维后保留类别可分性、可用作分类器。其局限性是最多降到 $K-1$ 维、需 $S_W$ 可逆（小样本问题需正则化）、假设较强（同协方差）、对异常值敏感。

## 分类规则

LDA 可直接用作分类器。假设各类同协方差 $\Sigma$，样本 $x$ 的判别函数为

$$ \delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log \pi_k $$

其中 $\pi_k$ 是类别 $k$ 的先验概率。预测为

$$ \hat{y} = \arg\max_k \delta_k(x) $$

该决策边界是线性的，故称"线性"判别分析。

若放宽同协方差假设，各类使用各自的 $\Sigma_k$，则得到二次判别分析（QDA），决策边界为二次曲面。

## 正则化

小样本情况下 $S_W$ 可能奇异，需正则化处理。

正则化 LDA（RLDA）对 $S_W$ 加入收缩项：

$$ S_W^{\text{reg}} = (1-\alpha) S_W + \alpha \cdot \text{diag}(S_W) $$

其中 $\alpha \in [0,1]$ 控制收缩程度。$\alpha=0$ 是标准 LDA，$\alpha=1$ 假设特征独立。

或加入单位矩阵：

$$ S_W^{\text{reg}} = S_W + \lambda I $$

其中 $\lambda > 0$ 是正则化参数，确保矩阵可逆。

另一种方法是先用 PCA 降维去除零特征值方向，再应用 LDA。

## 与 PCA 的比较

LDA 与 PCA 都是线性降维方法，但有本质区别：

PCA 是无监督的，最大化投影方差，保留数据的主要变化方向，但不保证类别可分性。

LDA 是有监督的，最大化类间散度与类内散度的比值，直接优化类别分离度。

PCA 最多可降到原始维度，LDA 最多降到 $K-1$ 维。

当类别信息可用且目标是分类时，LDA 通常优于 PCA；当探索数据结构或类别信息不可用时，PCA 更合适。两者可结合使用：先用 PCA 降维去除噪声，再用 LDA 进一步降维。
